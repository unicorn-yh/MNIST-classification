{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d3c258c-089e-49d5-a34f-0bcdd22eaf82",
   "metadata": {},
   "source": [
    "# 基于 Mindspore 框架的全连接网络梯度下降法（SGD）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b740f4-47b2-400e-afce-e3a489ef86e5",
   "metadata": {},
   "source": [
    "### 1. 导入模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2ca2c71-e56a-45be-8048-0977c02df43a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using MoXing-v2.1.0.5d9c87c8-5d9c87c8\n",
      "INFO:root:Using OBS-Python-SDK-3.20.9.1\n"
     ]
    }
   ],
   "source": [
    "import mindspore\n",
    "from mindspore import nn\n",
    "from mindspore import ops\n",
    "from mindspore.dataset import vision, transforms\n",
    "import os\n",
    "import numpy as np\n",
    "import moxing as mox\n",
    "import mindspore.dataset as ds\n",
    "from mindspore import ParameterTuple\n",
    "from collections.abc import Iterable\n",
    "from mindspore import context, DatasetHelper, save_checkpoint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860b0bb2-9e5b-45db-b6cf-0e223cb30f2d",
   "metadata": {},
   "source": [
    "### 2. 定义超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a09a6925-0a70-4ff7-befc-8e9a6aba66fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCH = 10\n",
    "LEARNING_RATE = 0.01\n",
    "MOMENTUM = 0.9\n",
    "NUM_CLASS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9ee918-d8df-41d3-998f-ce06b24e0305",
   "metadata": {},
   "source": [
    "### 3. 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d684850c-9c26-4031-a420-bc95139f9c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset:60000, Test dataset:10000\n"
     ]
    }
   ],
   "source": [
    "datasets_dir = '../datasets'\n",
    "if not os.path.exists(datasets_dir):\n",
    "    os.makedirs(datasets_dir)\n",
    "    \n",
    "if not os.path.exists(os.path.join(datasets_dir, 'MNIST_Data.zip')):\n",
    "    mox.file.copy('obs://modelarts-labs-bj4-v2/course/hwc_edu/python_module_framework/datasets/mindspore_data/MNIST_Data.zip', \n",
    "                  os.path.join(datasets_dir, 'MNIST_Data.zip'))\n",
    "    os.system('cd %s; unzip MNIST_Data.zip' % (datasets_dir))\n",
    "    \n",
    "# 读取完整训练样本和测试样本\n",
    "mnist_ds_train = ds.MnistDataset(os.path.join(datasets_dir, \"MNIST_Data/train\"))\n",
    "mnist_ds_test = ds.MnistDataset(os.path.join(datasets_dir, \"MNIST_Data/test\"))\n",
    "train_len = mnist_ds_train.get_dataset_size()\n",
    "test_len = mnist_ds_test.get_dataset_size()\n",
    "print(\"Train dataset:{}, Test dataset:{}\".format(train_len, test_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1a1d05-c0fa-46b4-9d75-0400e96fd97b",
   "metadata": {},
   "source": [
    "### 4. 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc4487a6-8600-4ac6-a8d4-7e264c919cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def datapipe(dataset, batch_size):\n",
    "    image_transforms = [\n",
    "        vision.c_transforms.Rescale(1.0 / 255.0, 0),\n",
    "        vision.c_transforms.Normalize(mean=(0.1307,), std=(0.3081,)),\n",
    "        vision.c_transforms.HWC2CHW()\n",
    "    ]\n",
    "    label_transform = transforms.c_transforms.TypeCast(mindspore.int32)\n",
    "\n",
    "    dataset = dataset.map(image_transforms, 'image')\n",
    "    dataset = dataset.map(label_transform, 'label')\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e896afed-2e0f-4cb1-92b0-79692ff5851c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datapipe(mnist_ds_train, BATCH_SIZE)\n",
    "test_dataset = datapipe(mnist_ds_test, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5eb8a5f4-406b-4c85-a70b-96deb918d67d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of image [N, C, H, W]: (64, 1, 28, 28) Float32\n",
      "Shape of label: (64,) Int32\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for image, label in test_dataset.create_tuple_iterator():\n",
    "    print(f\"Shape of image [N, C, H, W]: {image.shape} {image.dtype}\")\n",
    "    print(f\"Shape of label: {label.shape} {label.dtype}\")\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c58dd7-a4d2-4163-baf7-0e49ddee761c",
   "metadata": {},
   "source": [
    "### 5. 定义全连接网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90412e70-3ad3-45d1-a7dd-bc7a6539bd35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network<\n",
      "  (flatten): Flatten<>\n",
      "  (dense_relu_sequential): SequentialCell<\n",
      "    (0): Dense<input_channels=784, output_channels=256, has_bias=True>\n",
      "    (1): ReLU<>\n",
      "    (2): Dense<input_channels=256, output_channels=128, has_bias=True>\n",
      "    (3): ReLU<>\n",
      "    (4): Dense<input_channels=128, output_channels=10, has_bias=True>\n",
      "    >\n",
      "  >\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "class Network(nn.Cell):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dense_relu_sequential = nn.SequentialCell(\n",
    "            nn.Dense(28*28, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dense(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dense(128, NUM_CLASS)\n",
    "        )\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.dense_relu_sequential(x)\n",
    "        return logits\n",
    "\n",
    "net = Network()\n",
    "print(net)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5692668-37bc-43f9-818d-398fcb7b679e",
   "metadata": {},
   "source": [
    "### 6. 定义正向传播、反向传播和参数更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f72469bf-250e-4688-9c3f-f9aae15b05a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "_sum_op = ops.MultitypeFuncGraph(\"grad_sum_op\")\n",
    "_clear_op = ops.MultitypeFuncGraph(\"clear_op\")\n",
    "\n",
    "\n",
    "@_sum_op.register(\"Tensor\", \"Tensor\")\n",
    "def _cumulative_grad(grad_sum, grad):\n",
    "    \"\"\"Apply grad sum to cumulative gradient.\"\"\"\n",
    "    add = ops.AssignAdd()\n",
    "    return add(grad_sum, grad)\n",
    "\n",
    "\n",
    "@_clear_op.register(\"Tensor\", \"Tensor\")\n",
    "def _clear_grad_sum(grad_sum, zero):\n",
    "    \"\"\"Apply zero to clear grad_sum.\"\"\"\n",
    "    success = True\n",
    "    success = ops.depend(success, ops.assign(grad_sum, zero))\n",
    "    return success\n",
    "\n",
    "\n",
    "class TrainForwardBackward(nn.Cell):\n",
    "    def __init__(self, network, optimizer, grad_sum, sens=1.0):\n",
    "        super(TrainForwardBackward, self).__init__(auto_prefix=False)\n",
    "        self.network = network\n",
    "        self.network.set_grad()\n",
    "        self.network.add_flags(defer_inline=True)\n",
    "        self.weights = ParameterTuple(network.trainable_params())\n",
    "        self.optimizer = optimizer\n",
    "        self.grad_sum = grad_sum\n",
    "        self.grad = ops.GradOperation(get_by_list=True, sens_param=True)\n",
    "        self.sens = sens\n",
    "        self.hyper_map = ops.HyperMap()\n",
    "\n",
    "    def construct(self, *inputs):\n",
    "        weights = self.weights\n",
    "        loss = self.network(*inputs)\n",
    "        sens = ops.Fill()(ops.DType()(loss), ops.Shape()(loss), self.sens)\n",
    "        grads = self.grad(self.network, weights)(*inputs, sens)\n",
    "        return ops.depend(loss, self.hyper_map(ops.partial(_sum_op), self.grad_sum, grads))\n",
    "\n",
    "\n",
    "class TrainOptim(nn.Cell):\n",
    "    def __init__(self, optimizer, grad_sum):\n",
    "        super(TrainOptim, self).__init__(auto_prefix=False)\n",
    "        self.optimizer = optimizer\n",
    "        self.grad_sum = grad_sum\n",
    "\n",
    "    def construct(self):\n",
    "        return self.optimizer(self.grad_sum)\n",
    "\n",
    "\n",
    "class TrainClear(nn.Cell):\n",
    "    def __init__(self, grad_sum, zeros):\n",
    "        super(TrainClear, self).__init__(auto_prefix=False)\n",
    "        self.grad_sum = grad_sum\n",
    "        self.zeros = zeros\n",
    "        self.hyper_map = ops.HyperMap()\n",
    "\n",
    "    def construct(self):\n",
    "        success = self.hyper_map(ops.partial(_clear_op), self.grad_sum, self.zeros)\n",
    "        return success\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e68312-525b-4dcb-97a5-6d47f0d308d2",
   "metadata": {},
   "source": [
    "### 7. 定义模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af9d77cb-a86c-4fa8-b3f0-70a4308f345a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientAccumulation:\n",
    "    def __init__(self, network, loss_fn, optimizer):\n",
    "        self._network = network\n",
    "        self._loss_fn = loss_fn\n",
    "        self._optimizer = optimizer\n",
    "\n",
    "        params = self._optimizer.parameters\n",
    "        self._grad_sum = params.clone(prefix=\"grad_sum\", init='zeros')\n",
    "        self._zeros = params.clone(prefix=\"zeros\", init='zeros')\n",
    "        self._train_forward_backward = self._build_train_forward_backward_network()\n",
    "        self._train_optim = self._build_train_optim()\n",
    "        self._train_clear = self._build_train_clear()\n",
    "\n",
    "    @staticmethod\n",
    "    def _transform_callbacks(callbacks):\n",
    "        \"\"\"Transform callback to a list.\"\"\"\n",
    "        if callbacks is None:\n",
    "            return []\n",
    "\n",
    "        if isinstance(callbacks, Iterable):\n",
    "            return list(callbacks)\n",
    "\n",
    "        return [callbacks]\n",
    "\n",
    "    def _build_train_forward_backward_network(self):\n",
    "        \"\"\"Build forward and backward network\"\"\"\n",
    "        network = self._network\n",
    "        network = nn.WithLossCell(network, self._loss_fn)\n",
    "        loss_scale = 1.0\n",
    "        network = TrainForwardBackward(network, self._optimizer, self._grad_sum, loss_scale).set_train()\n",
    "        return network\n",
    "\n",
    "    def _build_train_optim(self):\n",
    "        \"\"\"Build optimizer network\"\"\"\n",
    "        network = TrainOptim(self._optimizer, self._grad_sum).set_train()\n",
    "        return network\n",
    "\n",
    "    def _build_train_clear(self):\n",
    "        \"\"\"Build clear network\"\"\"\n",
    "        network = TrainClear(self._grad_sum, self._zeros).set_train()\n",
    "        return network\n",
    "\n",
    "    def train_process(self, epoch, train_dataset, test_dataset):\n",
    "        \"\"\"\n",
    "        Training process. The data would be passed to network directly.\n",
    "        \"\"\"\n",
    "        train_helper = DatasetHelper(train_dataset, dataset_sink_mode=False, epoch_num=epoch)\n",
    "        test_helper = DatasetHelper(test_dataset, dataset_sink_mode=False, epoch_num=epoch)\n",
    "\n",
    "        for i in range(epoch):\n",
    "            #step = 0\n",
    "            sum_loss = 0.0\n",
    "            correct_train = 0\n",
    "            for k, next_element in enumerate(train_helper):\n",
    "                image, label = next_element\n",
    "                output = self._network(image)\n",
    "                pred = ops.Argmax(output_type=mindspore.int32)(output)\n",
    "                correct_train += (pred==label).asnumpy().sum().item()\n",
    "                loss = self._train_forward_backward(*next_element)\n",
    "                sum_loss += loss\n",
    "                self._train_optim()\n",
    "                self._train_clear()\n",
    "                \n",
    "            correct_test = 0\n",
    "            for k, next_element in enumerate(test_helper):\n",
    "                image, label = next_element\n",
    "                output = self._network(image)\n",
    "                pred = ops.Argmax(output_type=mindspore.int32)(output)\n",
    "                correct_test += (pred==label).asnumpy().sum().item()\n",
    "                \n",
    "            print(\"epoch:{}, \".format(i),end=\"\")\n",
    "            print(\"train acc:{:.4f}, \".format(float(correct_train)/train_len),end=\"\")\n",
    "            print(\"loss:{:.4f}, \".format(float(sum_loss)/train_len),end=\"\")\n",
    "            print(\"test acc:{:.4f}\".format(float(correct_test)/test_len))\n",
    "            train_dataset.reset()\n",
    "\n",
    "        save_checkpoint(self._train_forward_backward, \"gradient_accumulation.ckpt\", )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bd05b0-8e86-4087-9422-4ae809397069",
   "metadata": {},
   "source": [
    "### 8. 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "691f5673-e966-438a-9912-67bf76ee8789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== Starting Training ==================\n",
      "epoch:0, train acc:0.8137, loss:0.0091, test acc:0.9495\n",
      "epoch:1, train acc:0.9608, loss:0.0020, test acc:0.9673\n",
      "epoch:2, train acc:0.9746, loss:0.0013, test acc:0.9722\n",
      "epoch:3, train acc:0.9810, loss:0.0010, test acc:0.9742\n",
      "epoch:4, train acc:0.9861, loss:0.0007, test acc:0.9723\n",
      "epoch:5, train acc:0.9894, loss:0.0005, test acc:0.9794\n",
      "epoch:6, train acc:0.9922, loss:0.0004, test acc:0.9810\n",
      "epoch:7, train acc:0.9938, loss:0.0003, test acc:0.9766\n",
      "epoch:8, train acc:0.9955, loss:0.0002, test acc:0.9818\n",
      "epoch:9, train acc:0.9967, loss:0.0002, test acc:0.9819\n"
     ]
    }
   ],
   "source": [
    "context.set_context(mode=context.GRAPH_MODE, device_target='CPU')\n",
    "net_loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True, reduction=\"mean\")\n",
    "net_opt = nn.Momentum(net.trainable_params(), LEARNING_RATE, MOMENTUM)\n",
    "model = GradientAccumulation(net, net_loss, net_opt)\n",
    "\n",
    "print(\"================== Starting Training ==================\")\n",
    "model.train_process(EPOCH, train_dataset, test_dataset)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "mindspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
